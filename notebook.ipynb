{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "f5b01b70-49e0-4029-be4c-34081ba6ddd9",
        "_uuid": "a73d37659daa95989ae03f0a0dce5aed97a7a7f8",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c8dc0b32-f915-4fd5-95c7-93e63367b57e",
        "_uuid": "0df7b53ab362f1bae24b8cb53b7bd0768261fe9f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "#Tf-idf vectorizer from standard library"
    },
    {
      "metadata": {
        "_cell_guid": "40da1bca-33df-4dcc-818b-a1214ad76168",
        "_uuid": "30bfc24463fbc3cfc4f4589c8cffcf4b6e9ba4ab",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\ntfv = TfidfVectorizer(min_df=3, max_df=0.9, max_features=None, strip_accents='unicode',\\\n               analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), use_idf=1,\\\n               smooth_idf=1, sublinear_tf=1, stop_words='english')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a32e625e-156a-483c-882e-971247eb9f4c",
        "_uuid": "92ccf162098032caddbd2c97969ce22d893cb73e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train, test = pd.read_csv(\"../input/train.csv\"), pd.read_csv(\"../input/test.csv\")\nIDs = test['id']\nX_Train, X_Test = train['comment_text'], test['comment_text']\n#X_test.loc[X_test.isnull()] = \" \" # replace the 1 NaN value in test\nY_train = train[train.columns[2:]]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8fcc78fc-2f45-4d68-8674-189232d01e2a",
        "_uuid": "36e9be32e67f416edb922b0f2b72b65ffb6dc880",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f5bd3830-75dd-4cf4-8557-f10837d6cc91",
        "_uuid": "3941ee0879af8941d5b53ea06f1660b9484b245d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "type(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2680abcc-517d-4b42-b066-65fa9daed963",
        "_uuid": "5448aadea7038c4e0b8a6708d0f747b280606677",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import gc\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\n\ndel train\ndel test\ngc.collect()\n\nprint(\"%.2f of data is not flagged\" % (Y_train.loc[(Y_train.sum(axis=1) == 0)].shape[0] / Y_train.shape[0]))\n\n\n               \n\nprint(\"tfidf-vectorizing train ...\")\ntfv.fit(X_train)\nX_train = tfv.transform(X_train)\nprint(\"tfidf-vectorizing test ...\")\n\nX_test = tfv.transform(X_test)\n\nprint(\"fitting log reg & reporting cv accuracy ...\")\nfor i in range(Y_train.shape[1]):\n    feature = Y_train.columns[i]\n    print(\"\\n%s:\" % feature)\n    print(\"Baseline: %.2f\" % (Y_train.iloc[:,i].sum() / Y_train.shape[0]))\n    clf = LogisticRegression(C=4.0, solver='sag')\n    clf.fit(X_train, Y_train.iloc[:,i])\n    \n    print(cross_val_score(clf, X_train, Y_train.iloc[:,i], cv=5, scoring='f1'))\n    exec(\"pred_%s = pd.Series(clf.predict_proba(X_test).flatten()[1::2])\" % feature)\n\nsubmission = pd.DataFrame({\n             'id': IDs,\n             'toxic': pred_toxic,\n             'severe_toxic': pred_severe_toxic,\n             'obscene': pred_obscene,\n             'threat': pred_threat,\n             'insult': pred_insult,\n             'identity_hate': pred_identity_hate\n             })\n\n#submission.to_csv(\"submission_logreg.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fc37c1bd-c2d2-4a8f-9c68-19a82865cefd",
        "_uuid": "24001f4e5d493b890b811546f99b460b533c71bf"
      },
      "cell_type": "markdown",
      "source": "****Hashing Vectorizer using a hashing space of 2**18"
    },
    {
      "metadata": {
        "_cell_guid": "08bece36-f638-44da-8752-eaee1a935d70",
        "_uuid": "4e5996f3347fda510bacf3e7c2b013ffde70eda5",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "tfv=HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n                               alternate_sign=False)\n\nprint(\"tfidf-vectorizing train ...\")\ntfv.fit(X_train)\nX_train = tfv.transform(X_train)\nprint(\"tfidf-vectorizing test ...\")\n\nX_test = tfv.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8b4bd49d-f44d-4fd4-a05c-6ff38e328670",
        "_uuid": "1c0e0b0e095d28e077666413777a010df2df262b"
      },
      "cell_type": "markdown",
      "source": "****Prepare Input for Word2vec****"
    },
    {
      "metadata": {
        "_cell_guid": "31d1276c-b9d3-4c6b-8623-940971735b08",
        "_uuid": "8bebed34913807d6e82ea58b10d8d4c861e64f62",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import gensim\n\n# Load Google's pre-trained Word2Vec model\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "587a8c68-de78-44ac-be68-7cd89d1aff48",
        "_uuid": "544f1dd9ad9d24f758cb116cebbfb1a9ecf6030b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train, test = pd.read_csv(\"../input/train.csv\"), pd.read_csv(\"../input/test.csv\")\nIDs = test['id']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0822f8e5-3cac-46d1-83db-1cf2e4881f8f",
        "_uuid": "e4d743ec3aacbe40266ed5869aecabe60400d8d1",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2df650e1-3619-431d-b630-8d2097f8d737",
        "_uuid": "92c32e3968386093eab261d7a61ff73328579d9d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b877f81b-b38c-4047-aa99-75244afcf87c",
        "_uuid": "0c1216affa26a1e121844e09d4fcb9dd53880ed2",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bb86af9c-6809-4358-b1e9-7d05d598312d",
        "_uuid": "f31ee352f208eec16ba01a6d22ea90e437419dc7",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train=pd.DataFrame(train['comment_text'])\nX_test_comments=pd.DataFrame(test['comment_text'])\nX_total=pd.concat([X_train,X_test_comments])\nX_total_list=list(X_total['comment_text'])\nX_train_list=list(X_train['comment_text'])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7f0d38c1-4efe-428c-bfeb-4673df651346",
        "_uuid": "1c60f76a41f624da2c032935cacc776dcb34b2a4",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import gensim\nfrom gensim.models import Word2Vec\n\n# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n#model2 = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\ncontext_size = 7 # context window length\ndownsampling = 1e-5 # downsampling for very frequent words\nX_total_list_new=[]\nfor s in X_total_list:\n    if type(s) is float:\n        s=str(s)\n        X_total_list_new.append(s)\n    X_total_list_new.append(s)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "60665855-7d06-445d-9419-71d5fcdcc3d7",
        "_uuid": "86c386082ef8218cf7761fabc533cdf30753724d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model = gensim.models.Word2Vec([s.split() for s in X_total_list_new],size=200, min_count=6,workers=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "43569d3a-3502-459c-b02c-fc691fe42766",
        "_uuid": "547e30d8a5cd5e3af28622f83eb5cb92d20421b1",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model_2 = gensim.models.Word2Vec([s.split() for s in X_total_list_new],size=200, min_count=6,workers=4,window = context_size,\n    sample = downsampling\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6d0b1618-5678-4f9e-a239-9400f8e41ae8",
        "_uuid": "4af00ed091376650b005f4d52ec166e030f63ab0",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2b828566-9326-4099-9a68-59b425621874",
        "_uuid": "88f0be64e6355f2eb4c4878627073412345e9c30"
      },
      "cell_type": "markdown",
      "source": "**Most Frequent Words in training data **"
    },
    {
      "metadata": {
        "_cell_guid": "2d0c9736-b1d1-454c-a266-0b9933f73536",
        "_uuid": "cccba8e7003ea97c86619539282583df3753007a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(model_2.wv.index2word[0], model_2.wv.index2word[1], model_2.wv.index2word[2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5cece3bb-b51a-4e4a-912c-a77737ef228d",
        "_uuid": "2e1d2544ee890fa7a187cd15197aebe8e17b655e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "vocab_size = len(model.wv.vocab)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5806fa7f-a4a6-4020-b038-143622a64087",
        "_uuid": "6edf43f1d9c4a48f297c06cd8eee869d1dab0e63"
      },
      "cell_type": "markdown",
      "source": "**Least Frequent word int he vocabulary**"
    },
    {
      "metadata": {
        "_cell_guid": "65559a2f-6a7f-4c22-83e8-4ddc844cf440",
        "_uuid": "a11ed017c02b6e71094334234656ccd54662a225",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(model.wv.index2word[vocab_size-1], model.wv.index2word[vocab_size-2], model.wv.index2word[vocab_size-3])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e0a5891e-e7b7-4d9a-b239-ec9f8ce14f4f",
        "_uuid": "4535563b3c31b074c5621bbe2c15cfae964101dc"
      },
      "cell_type": "markdown",
      "source": "**Let's Prepare the Feature matrix using the trained features from word2vec on the dictionary**"
    },
    {
      "metadata": {
        "_cell_guid": "56083efd-d06a-4d0b-9169-cbb7ff3b1971",
        "_uuid": "492a54ed6d48252dc9fe2168ced2b9861d5e8c81"
      },
      "cell_type": "markdown",
      "source": "**First Prepare the embedding matrix and then the feature  matrix**"
    },
    {
      "metadata": {
        "_cell_guid": "31c8ae17-9d95-4208-8551-8ea081235bb3",
        "_uuid": "c96158c9ec0b83886e365b50e273ee32adeace8b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d669d06e-5489-4bdd-a918-6bf570e96ee0",
        "_uuid": "6501d8fa597d0fd8b5b5d11a7a70c8a64f19dd3e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(model.wv.vocab)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c5773783-08c9-49b2-9de0-6b725d86848b",
        "_uuid": "5c1487ffa03426f78e45a9a685799a18cc03b8ea",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# convert the wv word vectors into a numpy matrix that is suitable for insertion\n# into our TensorFlow and Keras models\nembedding_matrix = np.zeros((len(model.wv.vocab), 200))\nfor i in range(len(model.wv.vocab)):\n    embedding_vector = model.wv[model.wv.index2word[i]]\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nfeature_matrix = np.zeros((95851, 200))\ni=0\nfor s in X_train_list:\n    #embedding_vector=np.zeros(200)\n    initial_vector=np.zeros(200)\n    for word in s.split():\n    #for i in range(len(model.wv.vocab)):\n        if word in model.wv:\n            \n            embedding_vector=initial_vector+embedding_matrix[model.wv.vocab[word].index]\n            initial_vector=embedding_vector\n        #embedding\n    #if embedding_vector is not None:\n    feature_matrix[i] = embedding_vector\n    #print(\"one done\")\n    i=i+1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d00b1d06-30f4-48b9-9210-89f8c1418f35",
        "_uuid": "d4c36c4ed277934e4283a62f955ecc36ca55f8e9",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_frame=pd.DataFrame(feature_matrix)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c39dee2e-ebb2-4803-ad19-1ef60b001793",
        "_uuid": "2bc0c9eba8474e46bb39295b310c554b7dd78bf9",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_frame.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "34814aa8-7877-4517-ac9c-df705e8ab5ce",
        "_uuid": "ac93aabb39a40320c49247795156beae8fed8455"
      },
      "cell_type": "markdown",
      "source": "**Prepare the test data set **"
    },
    {
      "metadata": {
        "_cell_guid": "c94ef12e-a5d9-43e5-853a-4e03efcc26aa",
        "_uuid": "3a3f775d520a9237cee630c1987e4f66468b6304",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6146ce56-d8a1-4836-83df-af0ebcfca051",
        "_uuid": "8756da4db7c7a74a5bece6fff557e93d67b0a020",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_frame=pd.DataFrame(feature_matrix)\nX_test_list=list(test['comment_text'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "edc0e46b-23ec-4395-bbd3-496a75de7300",
        "_uuid": "a8779b035c0ebdd1ae28702c1cd7fd945a6f9e86",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "cc17ca2f-1720-484d-a202-49e4f098bcaa",
        "_uuid": "100b5cec05e9c724242867baf6cee81236bf7ed6",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# convert the wv word vectors into a numpy matrix that is suitable for insertion\n# into our TensorFlow and Keras models\n\ntest_feature_matrix = np.zeros((226998, 200))\ni=0\nfor s in X_test_list:\n    #embedding_vector=np.zeros(200)\n    initial_vector=np.zeros(200)\n    if type(s) is float:\n        s=str(s)\n    for word in s.split():\n    #for i in range(len(model.wv.vocab)):\n        if word in model.wv:\n            \n            embedding_vector=initial_vector+embedding_matrix[model.wv.vocab[word].index]\n            initial_vector=embedding_vector\n        #embedding\n    #if embedding_vector is not None:\n    test_feature_matrix[i] = embedding_vector\n    #print(\"one done\")\n    i=i+1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "33b3d8fb-581c-416c-bdb9-80bfa49c632d",
        "_uuid": "0fc1d57ca1a955771228a70ef00e408e57d1d66f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "test_features=pd.DataFrame(test_feature_matrix)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ec6c2a05-47b6-4e35-b7fa-7a4d1c88d9d4",
        "_uuid": "d21db492e1320b397711cf1edb4c6e4cfea4f49e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "test_features.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f75516e9-ff89-4fe3-9bee-ebb320b90fda",
        "_uuid": "cbe4b427e1a8bd7e296e1ccf51c436546cb476a1"
      },
      "cell_type": "markdown",
      "source": "**Now lets start with training linear models **"
    },
    {
      "metadata": {
        "_cell_guid": "09aa45f7-7e65-47a8-a3d4-c8f3455141f8",
        "_uuid": "a4bbba8c9186c4fa452dc2794a2a07d70715685c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Y_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7257525e-47f8-49f1-b39c-c1077d795bc2",
        "_uuid": "61d3d654307031c72b6a5d3fbd9dc2ed415ac2ab",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import gc\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfor i in range(Y_train.shape[1]):\n    feature = Y_train.columns[i]\n    print(\"\\n%s:\" % feature)\n    print(\"Baseline: %.2f\" % (Y_train.iloc[:,i].sum() / Y_train.shape[0]))\n    clf = LogisticRegression(C=4.0, solver='sag')\n    clf.fit(feature_frame, Y_train.iloc[:,i])\n    \n    print(cross_val_score(clf,feature_frame,Y_train.iloc[:,i], cv=5, scoring='f1'))\n    exec(\"pred_%s = pd.Series(clf.predict_proba(test_features).flatten()[1::2])\" % feature)\n\nsubmission = pd.DataFrame({\n             'id': IDs,\n             'toxic': pred_toxic,\n             'severe_toxic': pred_severe_toxic,\n             'obscene': pred_obscene,\n             'threat': pred_threat,\n             'insult': pred_insult,\n             'identity_hate': pred_identity_hate\n             })\n\nsubmission.to_csv(\"submission_logreg_adarsh.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5892abe3-3450-46be-b919-7c265f98d29f",
        "_uuid": "4b7897934d5797710b55de77d7dbb27485b84927",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nclf = svm.SVC()\n#clf.fit(X, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fe22c0d2-324c-4fb2-abe0-55d7ca97e758",
        "_uuid": "46f2a801fac63791284c0fe356b972f8ca59d9ac",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"adastrh\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7e2ef883-0826-4686-a16b-6c0bb25f28ba",
        "_uuid": "66ed29249a9d183003fc2ed3bfc36979633f9cda",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Y_train = train[train.columns[2:]]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0a65bdf8-3b27-40ba-8012-cbd5b1949636",
        "_uuid": "77771547bd7bc37b32ebbf209087925c0aadc6c2",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"hello\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6045468c-88e7-4094-90eb-8e302c369ca9",
        "_uuid": "a9a350766cfc676abcfdd8f5ea8d3029082c9601",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# lets prepare for the SVM classifiers and others\nfrom sklearn.model_selection import cross_val_score\nfor i in range(Y_train.shape[1]):\n    feature = Y_train.columns[i]\n    print(\"\\n%s:\" % feature)\n    print(\"Baseline: %.2f\" % (Y_train.iloc[:,i].sum() / Y_train.shape[0]))\n    #clf = LogisticRegression(C=4.0, solver='sag')\n    clf.fit(feature_frame, Y_train.iloc[:,i])\n    \n    print(cross_val_score(clf,feature_frame,Y_train.iloc[:,i], cv=5, scoring='f1'))\n    exec(\"pred_%s = pd.Series(clf.predict_proba(test_features).flatten()[1::2])\" % feature)\n\nsubmission = pd.DataFrame({\n             'id': IDs,\n             'toxic': pred_toxic,\n             'severe_toxic': pred_severe_toxic,\n             'obscene': pred_obscene,\n             'threat': pred_threat,\n             'insult': pred_insult,\n             'identity_hate': pred_identity_hate\n             })\n\nsubmission.to_csv(\"submission_svm_adarsh.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "660affa2-322c-4366-a3e3-dde30deb2dec",
        "_uuid": "69fbecbea10d855d85184a8f6c3132bee9d4276d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "#Adding Non-linearity "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "05c5903493f1018a1845af5459a877fa24025d32"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=5)\n\n\nfor i in range(0,Y_train.shape[1]):\n    for train, test in kfold.split(feature_frame, Y_train.iloc[:,i]):\n        print(i)\n        classifier = Sequential()\n        classifier.add(Dense(output_dim =180, init = 'uniform', activation = 'relu', input_dim = 200))\n        classifier.add(Dense(output_dim =180, init = 'uniform', activation = 'relu', input_dim = 200))\n        classifier.add(Dense(output_dim =180, init = 'uniform', activation = 'relu', input_dim = 200))\n        classifier.add(Dense(output_dim =180, init = 'uniform', activation = 'relu', input_dim = 200))\n        classifier.add(Dense(output_dim =180, init = 'uniform', activation = 'relu', input_dim = 200))\n        classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))    \n        \n        classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n        feature = Y_train.columns[i]\n        print(\"\\n%s:\" % feature)\n        print(\"Baseline: %.2f\" % (Y_train.iloc[:,i].sum() / Y_train.shape[0]))\n        classifier.fit(feature_frame.iloc[train], Y_train.iloc[:,i].iloc[train], batch_size = 10, nb_epoch = 2)\n        scores = classifier.evaluate(feature_frame.iloc[test], Y_train.iloc[:,i].iloc[test], verbose=0)\n        print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}